{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Novel-taxa simulated community generation and analysis\n",
    "\n",
    "[See issue request](https://github.com/BenKaehler/short-read-tax-assignment/issues/4)\n",
    "\n",
    "This notebook describes the generation of reference databases for novel-taxa simulated community analyses. In this analysis, random unique sequences are sampled from the reference database; all sequences sharing taxonomic affiliation at a given taxonomic level are removed from the reference database; and taxonomy is assigned to the query sequences at the given level. Thus, this test interrogates the behavior of a taxonomy classifier when challenged with \"novel\" sequences that are not represented by close matches within the reference sequence database. Such an analysis is performed to assess the degree to which \"overassignment\" occurs for sequences that are not represented in a reference database.\n",
    "\n",
    "The general framework for generating a modified reference database for this analysis consists of:\n",
    "\n",
    "1) Novel-taxa reference database generation: Remove empty taxa from ref dbs, split into query/reference subsets.\n",
    "\n",
    "2) Assign taxonomy to \"novel\" query sequences removed from the trimmed reference db to which it is paired.\n",
    "\n",
    "3) Measure rates of classification accuracy for each assignment method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "First step is to create a conda environment with the necessary dependencies. This requires installing [miniconda 3](http://conda.pydata.org/miniconda.html) to manage parallel python environments. After miniconda (or another conda version) is installed, proceed with [installing QIIME 2](https://docs.qiime2.org/2.0.6/install/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "* ``source`` = original reference database.\n",
    "* ``REF`` = ``source`` - ``novel`` seqs, used for taxonomy assignment.\n",
    "* ``QUERY`` = 'novel' query sequences randomly drawn from ``source``. \n",
    "* ``L`` = taxonomic level being tested\n",
    "    * 0 = kingdom, 1 = phylum, 2 = class, 3 = order, 4 = family, 5 = genus, 6 = species\n",
    "* ``branching`` = describes a taxon at level ``L`` that \"branches\" into two or more lineages at ``L + 1``. \n",
    "    * A \"branched\" taxon, then, describes these lineages. E.g., in the example below Lactobacillaceae, Lactobacillus, and Pediococcus branch, while Paralactobacillus is unbranching. The Lactobacillus and Pediococcus species are \"branched\". Paralactobacillus selangorensis is \"unbranched\"\n",
    "\n",
    "```\n",
    "Lactobacillaceae\n",
    "           └── Lactobacillus\n",
    "           │         ├── Lactobacillus brevis\n",
    "           │         └── Lactobacillus sanfranciscensis\n",
    "           ├── Pediococcus\n",
    "           │         ├── Pediococcus damnosus\n",
    "           │         └── Pediococcus claussenii\n",
    "           └── Paralactobacillus\n",
    "                     └── Paralactobacillus selangorensis\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# Define location of directory containing 'taxoneval.py'\n",
    "sys.path.append('/Users/nbokulich/Desktop/python_projects/taxoneval/taxoneval/')\n",
    "from taxoneval import *\n",
    "from eval_framework import *\n",
    "\n",
    "sys.path.append('/Users/nbokulich/Desktop/projects/short-read-tax-assignment/code/taxcompare')\n",
    "from framework_functions import *\n",
    "\n",
    "from os import path, makedirs, remove, rename\n",
    "from os.path import expandvars, exists, basename, splitext, dirname, join, isfile\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "from skbio.util import create_dir\n",
    "from skbio.alignment import global_pairwise_align_nucleotide, make_identity_substitution_matrix, local_pairwise_align_ssw\n",
    "from skbio.sequence import DNA\n",
    "from skbio import io, DNA\n",
    "from shutil import copyfile\n",
    "from itertools import product\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Novel-taxa reference data set generation\n",
    "\n",
    "This section describes the preparation of the data sets necessary for \"novel taxa\" analysis. The goals of this step are:\n",
    "1. Create a \"clean\" reference database that can be used for evaluation of \"novel taxa\" from phylum to species level.\n",
    "2. Generate simulated amplicons and randomly subsample query sequences to use as \"novel taxa\"\n",
    "3. Create modified sequence reference databases for taxonomic classification of \"novel taxa\" sequences\n",
    "\n",
    "In this first cell, we describe data set/database characteristics as a dictionary: dataset name is the key, with values reference sequence fasta, taxonomy, database name, forward primer sequence, reverse primer sequence, forward primer name, reverse primer name.\n",
    "\n",
    "MODIFY these values to generate novel-taxa files on a new reference database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "project_dir = expandvars(\"$HOME/Desktop/projects/short-read-tax-assignment\")\n",
    "analysis_name= \"novel-taxa-simulations\"\n",
    "data_dir = join(project_dir, \"data\", analysis_name)\n",
    "\n",
    "# List databases as fasta/taxonomy file pairs\n",
    "databases = {'B1-REF': [expandvars(\"$HOME/Desktop/ref_dbs/gg_13_8_otus/rep_set/97_otus.fasta\"), \n",
    "             expandvars(\"$HOME/Desktop/ref_dbs/gg_13_8_otus/taxonomy/97_otu_taxonomy.txt\"),\n",
    "             \"gg_13_8_otus\", \"GTGCCAGCMGCCGCGGTAA\", \"ATTAGAWACCCBDGTAGTCC\", \"515F\", \"806R\"],\n",
    "             'F1-REF': [expandvars(\"$HOME/Desktop/ref_dbs/unite-97-rep-set/97_otus.txt\"), \n",
    "             expandvars(\"$HOME/Desktop/ref_dbs/unite-97-rep-set/97_otu_taxonomy.txt\"), \n",
    "             \"unite-97-rep-set\", \"ACCTGCGGARGGATCA\", \"AACTTTYARCAAYGGAT\", \"BITSf\", \"B58S3r\"]\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will import these to a dataframe and view it. You should not need to modify the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reference file path</th>\n",
       "      <th>Reference tax path</th>\n",
       "      <th>Reference id</th>\n",
       "      <th>Fwd primer</th>\n",
       "      <th>Rev primer</th>\n",
       "      <th>Fwd primer id</th>\n",
       "      <th>Rev primer id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>B1-REF</th>\n",
       "      <td>/Users/nbokulich/Desktop/ref_dbs/gg_13_8_otus/...</td>\n",
       "      <td>/Users/nbokulich/Desktop/ref_dbs/gg_13_8_otus/...</td>\n",
       "      <td>gg_13_8_otus</td>\n",
       "      <td>GTGCCAGCMGCCGCGGTAA</td>\n",
       "      <td>ATTAGAWACCCBDGTAGTCC</td>\n",
       "      <td>515F</td>\n",
       "      <td>806R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-REF</th>\n",
       "      <td>/Users/nbokulich/Desktop/ref_dbs/unite-97-rep-...</td>\n",
       "      <td>/Users/nbokulich/Desktop/ref_dbs/unite-97-rep-...</td>\n",
       "      <td>unite-97-rep-set</td>\n",
       "      <td>ACCTGCGGARGGATCA</td>\n",
       "      <td>AACTTTYARCAAYGGAT</td>\n",
       "      <td>BITSf</td>\n",
       "      <td>B58S3r</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Reference file path  \\\n",
       "B1-REF  /Users/nbokulich/Desktop/ref_dbs/gg_13_8_otus/...   \n",
       "F1-REF  /Users/nbokulich/Desktop/ref_dbs/unite-97-rep-...   \n",
       "\n",
       "                                       Reference tax path      Reference id  \\\n",
       "B1-REF  /Users/nbokulich/Desktop/ref_dbs/gg_13_8_otus/...      gg_13_8_otus   \n",
       "F1-REF  /Users/nbokulich/Desktop/ref_dbs/unite-97-rep-...  unite-97-rep-set   \n",
       "\n",
       "                 Fwd primer            Rev primer Fwd primer id Rev primer id  \n",
       "B1-REF  GTGCCAGCMGCCGCGGTAA  ATTAGAWACCCBDGTAGTCC          515F          806R  \n",
       "F1-REF     ACCTGCGGARGGATCA     AACTTTYARCAAYGGAT         BITSf        B58S3r  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Arrange data set / database info in data frame\n",
    "simulated_community_definitions = pd.DataFrame.from_dict(databases, orient=\"index\")\n",
    "simulated_community_definitions.columns = [\"Reference file path\", \"Reference tax path\", \"Reference id\", \n",
    "                                           \"Fwd primer\", \"Rev primer\", \"Fwd primer id\", \"Rev primer id\"]\n",
    "simulated_community_definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate \"clean\" reference taxonomy and sequence database by removing taxonomy strings with empty or ambiguous levels'\n",
    "\n",
    "Set simulated community parameters, including amplicon length and the number of iterations to perform. Iterations will split our query sequence files into N chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B1-REF Sequence Counts\n",
      "Raw Fasta:            99322.0\n",
      "Clean Fasta:          6138.0\n",
      "Simulated Amplicons:  6107.0\n",
      "Simulated Reads:      6106.0\n",
      "B1-REF level 6 contains 1922 unique and 1075 branched taxa                  \n",
      "B1-REF level 5 contains 1111 unique and 1021 branched taxa                  \n",
      "B1-REF level 4 contains 252 unique and 183 branched taxa                  \n",
      "B1-REF level 3 contains 114 unique and 71 branched taxa                  \n",
      "B1-REF level 2 contains 61 unique and 45 branched taxa                  \n",
      "B1-REF level 1 contains 27 unique and 27 branched taxa                  \n",
      "F1-REF Sequence Counts\n",
      "Raw Fasta:            29435.0\n",
      "Clean Fasta:          20415.0\n",
      "Simulated Amplicons:  18085.0\n",
      "Simulated Reads:      12269.0\n",
      "F1-REF level 6 contains 8156 unique and 7276 branched taxa                  \n",
      "F1-REF level 5 contains 1746 unique and 1601 branched taxa                  \n",
      "F1-REF level 4 contains 377 unique and 315 branched taxa                  \n",
      "F1-REF level 3 contains 119 unique and 98 branched taxa                  \n",
      "F1-REF level 2 contains 34 unique and 30 branched taxa                  \n",
      "F1-REF level 1 contains 7 unique and 7 branched taxa                  \n"
     ]
    }
   ],
   "source": [
    "read_length = 250\n",
    "iterations = 3\n",
    "generate_novel_sequence_sets(simulated_community_definitions, data_dir, read_length, iterations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assign Taxonomy to Simulated Communities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set method and parameter combinations\n",
    "\n",
    "Define as a nested dictionary in the format:\n",
    "\n",
    "``{'method': {'parameter_name': [values, to, sweep]}}``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "method_parameters_combinations = { # probabalistic classifiers\n",
    "              'rdp': {'confidence': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]},\n",
    "              \n",
    "              # global alignment classifiers\n",
    "              'uclust': {'min_consensus_fraction': [0.51, 0.76, 1.0], \n",
    "                         'similarity': [0.8, 0.9],\n",
    "                         'uclust_max_accepts': [1, 3, 5]},\n",
    "             \n",
    "              # local alignment classifiers\n",
    "              'sortmerna': {'sortmerna_e_value': [1.0],\n",
    "                            'min_consensus_fraction': [0.51, 0.76, 1.0], \n",
    "                            'similarity': [0.8, 0.9],\n",
    "                            'sortmerna_best_N_alignments ': [1, 3, 5],\n",
    "                            'sortmerna_coverage' : [0.8, 0.9]}\n",
    "              # Let's scrap blast.\n",
    "              #'blast': {'blast_e_value': [10000.0, 0.001, 0.000000001]}\n",
    "             }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign taxonomy\n",
    "For each taxonomic level, loop through unique branched taxonomy strings\n",
    "        \n",
    "        1) Find sequence (QUERY), exclude from REF\n",
    "        2) Assign taxonomy to QUERY, using REF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define command structure\n",
    "# Parameters list: [parameter_output_dir, novel_query_fp, novel_ref_fp, novel_ref_taxonomy_fp, method, parameter_str]\n",
    "qiime1_command_template = \"mkdir -p {0} ; assign_taxonomy.py -v -i {1} -o {0} -r {2} -t {3} -m {4} {5} --rdp_max_memory 16000\"\n",
    "\n",
    "commands = []\n",
    "\n",
    "# Loop through data sets, taxonomic levels, iterations\n",
    "for index, data in simulated_community_definitions.iterrows():\n",
    "    simulated_community_dir = path.join(data_dir, index)\n",
    "    for level in range(6, 0, -1):\n",
    "        for iteration in range (0, iterations):\n",
    "            novel_query_fp = ''.join(map(str, [simulated_community_dir,\n",
    "                                               '/simulated_amplicon_QUERY_L',\n",
    "                                               level,'-iter', iteration, '.fna']))\n",
    "            \n",
    "            novel_ref_fp = ''.join(map(str, [simulated_community_dir, \n",
    "                                             '/simulated_amplicon_REF_L',\n",
    "                                             level,'-iter', iteration, '.fna']))\n",
    "            \n",
    "            novel_ref_taxonomy_fp = ''.join(map(str, [simulated_community_dir, \n",
    "                                                      '/simulated_amplicon_REF_TAXONOMY_L',\n",
    "                                                      level,'-iter', iteration, '.tsv']))\n",
    "\n",
    "            # Assign taxonomy to QUERY sequences\n",
    "            for method, parameters in method_parameters_combinations.items():\n",
    "                method_output_dir = join(simulated_community_dir, method)\n",
    "                parameter_ids = sorted(parameters.keys())\n",
    "                #parameter_ids.sort()\n",
    "                for parameter_combination in product(*[parameters[id_] for id_ in parameter_ids]):\n",
    "                    parameter_comb_id = ':'.join(map(str,parameter_combination))\n",
    "                    parameter_output_dir = join(method_output_dir, parameter_comb_id)\n",
    "                    taxonomy_assignment_fp = ''.join(map(str, [parameter_output_dir,\n",
    "                                                       '/simulated_amplicon_QUERY_L',\n",
    "                                                       level,'-iter', iteration, '_tax_assignments.txt']))\n",
    "                    if not exists(taxonomy_assignment_fp):\n",
    "                        parameter_str = ' '.join(['--%s %s' % e for e in zip(parameter_ids, parameter_combination)])\n",
    "                        command = qiime1_command_template.format(parameter_output_dir, novel_query_fp,\n",
    "                                                                 novel_ref_fp, novel_ref_taxonomy_fp, \n",
    "                                                                 method, parameter_str)\n",
    "                        commands.append(command)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, look at first command in command list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2340\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'mkdir -p /Users/nbokulich/Desktop/projects/taxoneval/novel/novel-taxa-simulations/B1-REF/sortmerna/0.51:0.8:1:0.8:1.0 ; assign_taxonomy.py -v -i /Users/nbokulich/Desktop/projects/taxoneval/novel/novel-taxa-simulations/B1-REF/simulated_amplicon_QUERY_L6-iter0.fna -o /Users/nbokulich/Desktop/projects/taxoneval/novel/novel-taxa-simulations/B1-REF/sortmerna/0.51:0.8:1:0.8:1.0 -r /Users/nbokulich/Desktop/projects/taxoneval/novel/novel-taxa-simulations/B1-REF/simulated_amplicon_REF_L6-iter0.fna -t /Users/nbokulich/Desktop/projects/taxoneval/novel/novel-taxa-simulations/B1-REF/simulated_amplicon_REF_TAXONOMY_L6-iter0.tsv -m sortmerna --min_consensus_fraction 0.51 --similarity 0.8 --sortmerna_best_N_alignments  1 --sortmerna_coverage 0.8 --sortmerna_e_value 1.0 --rdp_max_memory 16000'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(commands))\n",
    "commands[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run classification commands\n",
    "The cell below is what I used to run the legacy qiime classifiers. It works for these, since I only need a python-2 environment running qiime1, and then run this notebook on a kernel env running python-3. However, this will not work if, e.g., we need to access qiime1 and qiime2 methods, which necessarily run in separate environments. The second and third cells are untested so far but should do this. IT ALSO DOES NOT SUPPORT PARALLELIZATION."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for command in commands:\n",
    "    !{command}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "command_list = ' ; '.join(commands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: qiime [OPTIONS] COMMAND [ARGS]...\n",
      "\n",
      "Options:\n",
      "  --version  Print the version and exit.\n",
      "  --help     Show this message and exit.\n",
      "\n",
      "Commands:\n",
      "  info           Display information about QIIME.\n",
      "  tools          Tools for working with QIIME files.\n",
      "  diversity\n",
      "  emperor\n",
      "  feature-table\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$command_list\"\n",
    "source activate qiime-2\n",
    "echo $1\n",
    "# Go on, test it\n",
    "qiime --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create separate environment to call QIIME legacy classifiers\n",
    "\n",
    "This step is optional, and depends largely on the classifiers being used for taxonomy assignment and their individual dependencies. In this example, we want to create a python-2 environment in which to run QIIME-1's legacy classifiers: RDP and UCLUST classifier.\n",
    "\n",
    "This step will not always be necessary, only in cases where any chosen classifier requires dependencies that conflict with this framework, including different versions of python.\n",
    "\n",
    "This step requires installing [miniconda 3](http://conda.pydata.org/miniconda.html) to manage parallel python environments. After miniconda (or another conda version) is installed, proceed with [installing QIIME 1](http://qiime.org/install/install.html):\n",
    "\n",
    "To activate qiime1, type:\n",
    "\n",
    "``source activate qiime1``\n",
    "\n",
    "\n",
    "**NOTE** that we activate the newly created qiime1 conda environment on the first line of this cell. When changing this cell to run other taxonomy classifiers, this will need to be changed to the environment that carries the necessary packages/dependencies, or removed altogether if the classifier of interest can be run in a python-3 environment and has no dependency conflicts with the QIIME 2 environment required for the rest of the notebook.\n",
    "\n",
    "First, in a new terminal window, enter the following command to start your parallel engines:\n",
    "\n",
    "``ipcluster start -n 4``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nbokulich/miniconda3/envs/taxa-dev/lib/python3.5/site-packages/ipyparallel/client/client.py:442: RuntimeWarning: \n",
      "            Controller appears to be listening on localhost, but not on this machine.\n",
      "            If this is true, you should specify Client(...,sshserver='you@192.168.1.2')\n",
      "            or instruct your controller to listen on an external IP.\n",
      "  RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "### **** I could use some help reviewing this — this and the following cell seem to run without error\n",
    "### **** (the box below is a \"warning\" according to Client(), which appears if run on laptop with variable IP)\n",
    "### **** However, no files are output. Am I setting up ipyparallel correctly?\n",
    "\n",
    "# https://ipyparallel.readthedocs.io/en/latest/process.html#parallel-process\n",
    "\n",
    "!source activate qiime1\n",
    "\n",
    "from ipyparallel import Client\n",
    "rc = Client()\n",
    "lview = rc.load_balanced_view()\n",
    "\n",
    "@lview.parallel()\n",
    "def call_cmd(cmd):\n",
    "    from qcli import qcli_system_call\n",
    "    stdout, stderr, retval = qcli_system_call(cmd)\n",
    "    # return stdout, stderr, the return value, and the command\n",
    "    # the command is useful in case it needs to be re-run\n",
    "    return stdout, stderr, retval, cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This cell does not work, as source and commands need to run in same bash instance. However, I am leaving this here\n",
    "# to remind us of the problem: how do we run bash commands in parallel? It is not ideal to `source` a new env at each\n",
    "# call, as it is time-consuming. Is ipyparallel even the way to do this?\n",
    "!source activate qiime1\n",
    "\n",
    "r = call_cmd.map(commands)\n",
    "\n",
    "!source activate taxa-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Other possibilities:\n",
    "\n",
    "# run processes 1-4, wait until complete to run next set\n",
    "process1 &\n",
    "process2 &\n",
    "process3 &\n",
    "process4 &\n",
    "wait\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score classification accuracy\n",
    "\n",
    "The key measure here is rate of ``match`` vs. ``overclassification``, hence P/R/F are not useful metrics. Instead, define and measure the following as percentages:\n",
    "* Match vs. overclassification rate\n",
    "    * Match: assignment == L - 1 (e.g., a novel species is assigned the correct genus)\n",
    "    * overclassification: assignment == L (e.g., correct genus but assigns to a near neighbor)\n",
    "    * misclassification: incorrect assignment at L - 1 (e.g., wrong genus-level assignment)\n",
    "    \n",
    "Where ``L`` = taxonomic level being tested\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "# Access assignment data\n",
    "for index, data in simulated_community_definitions.iterrows():\n",
    "    simulated_community_dir = path.join(data_dir, index)\n",
    "    for level in range(6, 0, -1):\n",
    "        for iteration in range (0, iterations):\n",
    "            novel_query_taxonomy_fp = ''.join(map(str, [simulated_community_dir, \n",
    "                                                        '/simulated_amplicon_QUERY_TAXONOMY_L',\n",
    "                                                        level,'-iter', iteration, '.tsv']))\n",
    "            \n",
    "            novel_query_fp = ''.join(map(str, [simulated_community_dir,\n",
    "                                               '/simulated_amplicon_QUERY_L',\n",
    "                                               level,'-iter', iteration, '.fna']))\n",
    "            \n",
    "            for method, parameters in method_parameters_combinations.items():\n",
    "                method_output_dir = join(simulated_community_dir, method)\n",
    "                parameter_ids = sorted(parameters.keys())\n",
    "                for parameter_combination in product(*[parameters[id_] for id_ in parameter_ids]):\n",
    "                    parameter_comb_id = ':'.join(map(str,parameter_combination))\n",
    "                    parameter_output_dir = join(method_output_dir, parameter_comb_id)\n",
    "\n",
    "                    # Loop through taxonomy assignments\n",
    "                    count = 0\n",
    "                    match = 0\n",
    "                    overclassification = 0\n",
    "                    underclassification = 0\n",
    "                    misclassification = 0\n",
    "                    \n",
    "                    # Create empty list of levels at which first mismatch occurs for each query (0 = kingdom, 7 = NA)\n",
    "                    mismatch_level_list = [0, 0 , 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "                    # Create log file\n",
    "                    log_fp = ''.join(map(str, [parameter_output_dir, '/classification_accuracy_log_L', \n",
    "                                               level,'-iter', iteration, '.tsv']))\n",
    "                    logfile = open(log_fp, 'w')\n",
    "\n",
    "                    logfile.write('index\\tlevel\\titeration\\tmethod\\tparameter_combination\\tseq_id\\\n",
    "                    \\ttaxonomy_assignment\\tquery_taxonomy\\tresult\\tmismatch_level\\n')\n",
    "                    \n",
    "                    # access \"observed taxonomy\"\n",
    "                    taxonomy_assignment_fp = ''.join(map(str, [parameter_output_dir,\n",
    "                                                       '/simulated_amplicon_QUERY_L',\n",
    "                                                       level,'-iter', iteration, '_tax_assignments.txt']))\n",
    "                    for line in open(taxonomy_assignment_fp, 'r'):\n",
    "                        count += 1\n",
    "                        # Read seq id and taxonomy from line; as different assigners output different results,\n",
    "                        # cannot use the original line splitter (below).\n",
    "                        #(seq_id, taxonomy_assignment, confidence, matches) = line.split('\\t')\n",
    "                        id_taxonomy_confidence_line = line.split('\\t')\n",
    "                        seq_id = id_taxonomy_confidence_line[0]\n",
    "                        taxonomy_assignment = id_taxonomy_confidence_line[1]\n",
    "\n",
    "                         # access \"expected taxonomy\"\n",
    "                        for novel_query_taxonomy_line in open(novel_query_taxonomy_fp, 'r'):\n",
    "                            novel_query_taxonomy_line = novel_query_taxonomy_line.rstrip()\n",
    "                            (query_id, query_taxonomy) = novel_query_taxonomy_line.split('\\t')\n",
    "                            if seq_id == query_id:\n",
    "                                query_taxa = query_taxonomy.split(';')\n",
    "                                assignment_taxa = taxonomy_assignment.split(';')\n",
    "                                \n",
    "                                # Find shallowest level of mismatch\n",
    "                                mismatch_level = 0\n",
    "                                for taxa_comparison in zip(assignment_taxa, query_taxa):\n",
    "                                    # Remove whitespace from around taxa names to support different classifier formats\n",
    "                                    if taxa_comparison[0].strip() == taxa_comparison[1].strip():\n",
    "                                        mismatch_level += 1\n",
    "                                    else:\n",
    "                                        break\n",
    "                                mismatch_level_list[mismatch_level] += 1\n",
    "                                \n",
    "                                # if observed has same assignment depth as expected and top level matches, ==match\n",
    "                                # len(query_taxa) - 1 because query_taxa is actual taxonomy string, L-1 is expected\n",
    "                                if len(assignment_taxa) == len(query_taxa) - 1 and \\\n",
    "                                assignment_taxa[level - 1].strip() == query_taxa[level - 1].strip():\n",
    "                                    match += 1\n",
    "                                    result = 'match'\n",
    "                                # if deeper and assignemnt at L-1 is correct, count as overclassification\n",
    "                                elif len(assignment_taxa) >= len(query_taxa) and \\\n",
    "                                assignment_taxa[level - 1].strip() == query_taxa[level - 1].strip():\n",
    "                                    overclassification += 1\n",
    "                                    result = 'overclassification'\n",
    "                                # if shallower and top-level assignment is correct, count as underclassification\n",
    "                                elif len(assignment_taxa) < len(query_taxa) - 1 and \\\n",
    "                                assignment_taxa[len(assignment_taxa)-1].strip() == query_taxa[len(assignment_taxa)-1].strip():\n",
    "                                    underclassification += 1\n",
    "                                    result = 'underclassification'\n",
    "                                # Otherwise, count as misclassification\n",
    "                                else:\n",
    "                                    misclassification += 1\n",
    "                                    result = 'misclassification'\n",
    "                                    \n",
    "                                logfile.write('{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\t{6}\\t{7}\\t{8}\\t{9}\\n'.format( \\\n",
    "                                               index, level, iteration, method, parameter_comb_id, \n",
    "                                               seq_id, taxonomy_assignment, query_taxonomy, result, mismatch_level))\n",
    "                                break\n",
    "                    logfile.close()\n",
    "                                \n",
    "                    # tally score ratios\n",
    "                    match_ratio = match / count\n",
    "                    overclassification_ratio = overclassification / count\n",
    "                    underclassification_ratio = underclassification / count\n",
    "                    misclassification_ratio = misclassification / count\n",
    "                    \n",
    "                    results.append('\\t'.join(map(str, [index, level, iteration, method, parameter_comb_id, \n",
    "                                                       match_ratio, overclassification_ratio, \n",
    "                                                       underclassification_ratio, misclassification_ratio,\n",
    "                                                       mismatch_level_list])))\n",
    "\n",
    "# Write out results to file\n",
    "summary_fp = join(data_dir, 'classification_accuracy_summary.tsv')\n",
    "summary = open(summary_fp, 'w')\n",
    "\n",
    "summary.write('index\\tlevel\\titeration\\tmethod\\tparameter_combination\\\n",
    "\\tmatch_ratio\\toverclassification_ratio\\tunderclassification_ratio\\tmisclassification_ratio\\tmismatch_level_list\\n')\n",
    "\n",
    "for resultsline in results:\n",
    "    summary.write('{0}\\n'.format(resultsline))\n",
    "    \n",
    "summary.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells can be used to specify specific files to run/re-run (e.g., in case of missing files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assign_taxonomy.py -v -i /Users/nbokulich/Desktop/projects/taxoneval/novel/novel-taxa-simulations/B1-REF/simulated_amplicon_QUERY_L4-iter2.fna -o /Users/nbokulich/Desktop/projects/taxoneval/novel/novel-taxa-simulations/B1-REF/rdp/0.8 -r /Users/nbokulich/Desktop/projects/taxoneval/novel/novel-taxa-simulations/B1-REF/simulated_amplicon_REF_L4-iter2.fna -t /Users/nbokulich/Desktop/projects/taxoneval/novel/novel-taxa-simulations/B1-REF/simulated_amplicon_REF_TAXONOMY_L4-iter2.tsv -m rdp --confidence 0.8 --rdp_max_memory 16000\n"
     ]
    }
   ],
   "source": [
    "# Set these variables\n",
    "index = 'B1-REF'\n",
    "iteration = 2\n",
    "level = 4\n",
    "method = 'rdp'\n",
    "parameter_combination = 0.8\n",
    "parameter_str = '--confidence 0.8'\n",
    "\n",
    "# Do not change\n",
    "simulated_community_dir = path.join(data_dir, index)\n",
    "novel_query_taxonomy_fp = ''.join(map(str, [simulated_community_dir, '/simulated_amplicon_QUERY_TAXONOMY_L',\n",
    "                                            level,'-iter', iteration, '.tsv']))            \n",
    "novel_query_fp = ''.join(map(str, [simulated_community_dir, '/simulated_amplicon_QUERY_L',\n",
    "                                   level,'-iter', iteration, '.fna']))\n",
    "novel_ref_fp = ''.join(map(str, [simulated_community_dir, '/simulated_amplicon_REF_L',\n",
    "                                 level,'-iter', iteration, '.fna']))\n",
    "novel_ref_taxonomy_fp = ''.join(map(str, [simulated_community_dir,'/simulated_amplicon_REF_TAXONOMY_L',\n",
    "                                          level,'-iter', iteration, '.tsv']))\n",
    "method_output_dir = join(simulated_community_dir, method)\n",
    "parameter_output_dir = join(method_output_dir, parameter_comb_id)\n",
    "\n",
    "qiime1_command_template = \"assign_taxonomy.py -v -i {1} -o {0} -r {2} -t {3} -m {4} {5} --rdp_max_memory 16000\"\n",
    "command = qiime1_command_template.format(parameter_output_dir, novel_query_fp, novel_ref_fp, novel_ref_taxonomy_fp, \n",
    "                                                                 method, parameter_str)\n",
    "print(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assign_taxonomy.py -v -i /Users/nbokulich/Desktop/projects/taxoneval/novel/novel-taxa-simulations/B1-REF/simulated_amplicon_QUERY_L4-iter2.fna -o /Users/nbokulich/Desktop/projects/taxoneval/novel/novel-taxa-simulations/B1-REF/rdp/0.8 -r /Users/nbokulich/Desktop/projects/taxoneval/novel/novel-taxa-simulations/B1-REF/simulated_amplicon_REF_L4-iter2.fna -t /Users/nbokulich/Desktop/projects/taxoneval/novel/novel-taxa-simulations/B1-REF/simulated_amplicon_REF_TAXONOMY_L4-iter2.tsv -m rdp --confidence 0.8 --rdp_max_memory 16000\r\n"
     ]
    }
   ],
   "source": [
    "!echo $command; $command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot classification accuracy\n",
    "\n",
    "Some other day..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
